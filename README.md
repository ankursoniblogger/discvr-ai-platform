# DiscvrAI â€” AI-Powered Product Discovery

## ğŸš€ Live Demo

| Service | URL |
|--------|------|
| ğŸŒ Frontend | https://discvr-ai-platform.vercel.app |
| ğŸ”Œ Backend API | https://discvr-ai-backend.onrender.com |
| â¤ï¸ Health Check | https://discvr-ai-backend.onrender.com/api/health |


A full-stack **Product Discovery** application with an AI/LLM-powered natural-language search. Browse a curated electronics catalog, or ask the AI to find exactly what you need â€” "Show me budget laptops" or "What's good for gaming?"

## ğŸ—ï¸ Architecture

```
Discvrai/
â”œâ”€â”€ backend/                  # Express API + OpenAI integration
â”‚   â”œâ”€â”€ data/products.json    # Mock product catalog (8 products)
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ products.js       # GET /api/products (list + filter)
â”‚   â”‚   â””â”€â”€ ask.js            # POST /api/ask (LLM-powered search)
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ llmService.js     # OpenAI prompt + response parsing
â”‚   â”œâ”€â”€ server.js             # Express entry point
â”‚   â”œâ”€â”€ .env.example          # Env var template
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ frontend/                 # React + Vite
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/       # Reusable UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ Header.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ AskBox.jsx    # Natural-language search input
â”‚   â”‚   â”‚   â”œâ”€â”€ AISummary.jsx # LLM summary display
â”‚   â”‚   â”‚   â”œâ”€â”€ ProductCard.jsx  # Reusable product card
â”‚   â”‚   â”‚   â””â”€â”€ ProductList.jsx  # Grid of ProductCards
â”‚   â”‚   â”œâ”€â”€ services/api.js   # Centralised API helpers
â”‚   â”‚   â”œâ”€â”€ App.jsx           # Root component (state management)
â”‚   â”‚   â””â”€â”€ main.jsx          # ReactDOM entry
â”‚   â”œâ”€â”€ vite.config.js        # Vite config + API proxy
â”‚   â””â”€â”€ package.json
â””â”€â”€ README.md                 # â† you are here
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Node.js** â‰¥ 18
- **npm** â‰¥ 9
- An **OpenAI API key** (or compatible provider)

### 1. Clone the repo

```bash
git clone https://github.com/<your-username>/discvrai.git
cd discvrai
```

### 2. Backend setup

```bash
cd backend
npm install

# Create your .env from the template
cp .env.example .env
# Then edit .env and paste your OpenAI key:
#   OPENAI_API_KEY=sk-...
```

### 3. Frontend setup

```bash
cd ../frontend
npm install
```

### 4. Run both servers

Open **two terminals**:

```bash
# Terminal 1 â€” Backend (port 5000)
cd backend
npm run dev          # or: npm start

# Terminal 2 â€” Frontend (port 3000)
cd frontend
npm run dev
```

Open **http://localhost:3000** in your browser.

---

## ğŸ“¡ API Endpoints

| Method | Path             | Description                                          |
| ------ | ---------------- | ---------------------------------------------------- |
| `GET`  | `/api/products`  | List all products. Optional query params: `category`, `q` (keyword). |
| `POST` | `/api/ask`       | AI search. Body: `{ "query": "your question" }`. Returns `{ products, summary }`. |
| `GET`  | `/api/health`    | Health check.                                        |

### Example â€” Ask AI

```bash
curl -X POST http://localhost:5000/api/ask \
  -H "Content-Type: application/json" \
  -d '{"query": "Show me budget laptops"}'
```

**Response:**

```json
{
  "products": [
    { "id": 1, "name": "ProBook Lite 14", "category": "Laptops", "price": 499, "..." : "..." }
  ],
  "summary": "The ProBook Lite 14 is a lightweight, budget-friendly laptop ideal for students and productivity tasks at just $499."
}
```

---

## ğŸ¤– AI / LLM Integration

- **Provider:** OpenAI (gpt-3.5-turbo) â€” easily swappable to gpt-4 or any compatible API.
- **Prompt design:** The system prompt sends the entire product catalog (ID, name, category, price, tags, description) as context, then instructs the model to return structured JSON: `{ "productIds": [...], "summary": "..." }`.
- **Structured output:** The backend parses the LLM response as JSON, validates the shape, resolves product IDs to full objects, and returns a clean response.
- **Error handling:** Rate limits â†’ 503, invalid key â†’ 502, parse failures â†’ 502. No raw API errors or keys are ever exposed to the client.

---

## âš™ï¸ Environment Variables

| Variable         | Required | Description                            |
| ---------------- | -------- | -------------------------------------- |
| `OPENAI_API_KEY` | âœ…       | Your OpenAI API key                    |
| `PORT`           | âŒ       | Backend port (default: `5000`)         |

> **Security:** `.env` is git-ignored. Never commit your API key.

---

## ğŸ§© Key Technical Decisions

| Area                  | Choice                                          |
| --------------------- | ----------------------------------------------- |
| Backend framework     | Express (lightweight, fast to set up)            |
| Frontend framework    | React 18 + Vite (fast HMR, modern tooling)       |
| LLM provider          | OpenAI Chat Completions API                      |
| Data store            | In-memory JSON file (no DB needed for scope)     |
| Styling               | Vanilla CSS with CSS custom properties (dark theme) |
| API proxy             | Vite dev server proxy (`/api` â†’ `localhost:5000`) |
| Reusable component    | `ProductCard` â€” used in both catalog and AI results |

---

## ğŸ”® What I'd Add with More Time

- **Product detail page** with dynamic route (`/products/:id`) and an AI-generated one-liner
- **Response caching** to avoid redundant LLM calls for similar queries
- **Streaming** LLM responses for a chat-like UX
- **Testing** â€” Jest + React Testing Library for components, Supertest for API
- **Deployment** â€” Vercel (frontend) + Railway (backend)
- **Semantic search** using embeddings for more accurate product matching

---

## â±ï¸ Time Spent

~2.5 hours (backend + LLM: ~1h, frontend + integration: ~1h, README + polish: ~30min)
